{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf485df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing useful modeuls\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9240ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"stage1_train\"\n",
    "folders = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.reduce = nn.Conv2d(in_ch, in_ch // 2, kernel_size=1, bias=False)\n",
    "        up_ch = in_ch // 2\n",
    "\n",
    "        self.conv = DoubleConv(up_ch + skip_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = self.reduce(x)\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetLiteDensity(nn.Module):\n",
    "    def __init__(self, in_channels=1, base=16, out_activation=\"relu\", patch_size=256):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = base, base*2, base*4, base*8, base*16\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, c1)\n",
    "        self.d1  = Down(c1, c2)\n",
    "        self.d2  = Down(c2, c3)\n",
    "        self.d3  = Down(c3, c4)\n",
    "        self.d4  = Down(c4, c5)\n",
    "\n",
    "        self.u1 = Up(c5, c4, c4)\n",
    "        self.u2 = Up(c4, c3, c3)\n",
    "        self.u3 = Up(c3, c2, c2)\n",
    "        self.u4 = Up(c2, c1, c1)\n",
    "\n",
    "        self.outc1 = nn.Conv2d(c1, 1, kernel_size=1)\n",
    "        self.outc2 = nn.Conv2d(1, 1, kernel_size=patch_size)\n",
    "\n",
    "        if out_activation == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif out_activation == \"softplus\":\n",
    "            self.act = nn.Softplus()\n",
    "        else:\n",
    "            print(\"WRONG THING\")\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.d1(x1)\n",
    "        x3 = self.d2(x2)\n",
    "        x4 = self.d3(x3)\n",
    "        x5 = self.d4(x4)\n",
    "\n",
    "        x = self.u1(x5, x4)\n",
    "        x = self.u2(x,  x3)\n",
    "        x = self.u3(x,  x2)\n",
    "        x = self.u4(x,  x1)\n",
    "\n",
    "        x = self.outc1(x)\n",
    "        x = self.outc2(x)\n",
    "        return self.act(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff3c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_index_tiled(root, folder_ids, P=256):\n",
    "    patch_index = []\n",
    "    for sample_id in folder_ids:\n",
    "        sample_dir = os.path.join(root, sample_id)\n",
    "        x_full = np.load(os.path.join(sample_dir, \"image.npy\"), mmap_mode=\"r\")\n",
    "        H, W = x_full.shape\n",
    "\n",
    "        n_h = (H + P - 1) // P\n",
    "        n_w = (W + P - 1) // P\n",
    "\n",
    "        for i in range(n_h):\n",
    "            top = i * P\n",
    "            for j in range(n_w):\n",
    "                left = j * P\n",
    "                patch_index.append((sample_id, top, left))\n",
    "\n",
    "    return patch_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9392c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, root, patch_indices, P=256, invert=True):\n",
    "        self.root = root\n",
    "        self.patch_indices = patch_indices\n",
    "        self.P = P\n",
    "        self.invert = invert\n",
    "\n",
    "    def __len__(self):\n",
    "        n = len(self.patch_indices)\n",
    "        return 2 * n if self.invert else n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        n = len(self.patch_indices)\n",
    "        inverted = self.invert and (idx >= n)\n",
    "        if inverted:\n",
    "            idx = idx - n\n",
    "\n",
    "        sample_id, top, left = self.patch_indices[idx]\n",
    "        sample_dir = os.path.join(self.root, sample_id)\n",
    "\n",
    "        x_full = np.load(os.path.join(sample_dir, \"image.npy\"), mmap_mode=\"r\")\n",
    "        y_full = np.load(os.path.join(sample_dir, \"density_varfactor0.2.npy\"), mmap_mode=\"r\")\n",
    "\n",
    "        x_shape, y_shape = x_full.shape, y_full.shape\n",
    "\n",
    "        assert x_shape == y_shape\n",
    "        assert x_full.ndim == 2 and y_full.ndim == 2\n",
    "\n",
    "        H, W = x_shape\n",
    "\n",
    "        H_pad = ((H + self.P - 1) // self.P) * self.P\n",
    "        W_pad = ((W + self.P - 1) // self.P) * self.P\n",
    "\n",
    "        pad_h = H_pad - H\n",
    "        pad_w = W_pad - W\n",
    "\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x_full = np.pad(x_full, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)\n",
    "            y_full = np.pad(y_full, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "        x = x_full[top:top+self.P, left:left+self.P].astype(np.float32, copy=False)\n",
    "        y = y_full[top:top+self.P, left:left+self.P].astype(np.float32, copy=False)\n",
    "\n",
    "        assert x.shape == (self.P, self.P) and y.shape == (self.P, self.P)\n",
    "\n",
    "        if inverted:\n",
    "            x = 1.0 - x\n",
    "\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        y = torch.from_numpy(y).unsqueeze(0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 256\n",
    "\n",
    "def split_train_val(folder_ids, val_frac=0.2, seed=123):\n",
    "\n",
    "    folder_ids = list(folder_ids)\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(folder_ids)\n",
    "    n_val = int(round(len(folder_ids) * val_frac))\n",
    "    val_ids = sorted(folder_ids[:n_val])\n",
    "    train_ids = sorted(folder_ids[n_val:])\n",
    "    \n",
    "    return train_ids, val_ids\n",
    "\n",
    "train_ids, val_ids = split_train_val(folders, val_frac=0.2, seed=123)\n",
    "train_patch_indices = build_patch_index_tiled(root, train_ids, P=P)\n",
    "val_patch_indices   = build_patch_index_tiled(root, val_ids, P=P)\n",
    "\n",
    "train_dataset = PatchDataset(root, train_patch_indices, P=P, invert=True)\n",
    "val_dataset = PatchDataset(root, val_patch_indices, P=P, invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46005dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=4,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa45c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_huber(y_pred, y_true):\n",
    "    pred_c = y_pred.sum(dim=(1,2,3))\n",
    "    true_c = y_true.sum(dim=(1,2,3))\n",
    "    return F.smooth_l1_loss(pred_c, true_c)\n",
    "\n",
    "def total_loss(y_pred, y_true):\n",
    "    cnt = count_huber(y_pred, y_true)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae5c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset len: 3590\n",
      "Val dataset len: 858\n",
      "Train batches per epoch: 898\n",
      "Val batches per epoch: 215\n",
      "{'lr': 0.0001, 'weight_decay': 0.0001}\n",
      "\n",
      "Epoch 0 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salla\\AppData\\Local\\Temp\\ipykernel_177108\\1967232452.py:50: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\b\\abs_34s6o8i12c\\croot\\libtorch_1751464457133\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  y = torch.from_numpy(y).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 0/898 MAE=5.3762 totloss=4.934874534606934\n",
      "  step 50/898 MAE=3.1027 totloss=2.619500160217285\n",
      "  step 100/898 MAE=12.8667 totloss=12.468442916870117\n",
      "  step 150/898 MAE=5.1214 totloss=4.674820423126221\n",
      "  step 200/898 MAE=13.2633 totloss=12.816695213317871\n",
      "  step 250/898 MAE=3.4584 totloss=3.083408832550049\n",
      "  step 300/898 MAE=15.0349 totloss=14.55339527130127\n",
      "  step 350/898 MAE=3.7759 totloss=3.275911808013916\n",
      "  step 400/898 MAE=6.4775 totloss=5.977530479431152\n",
      "  step 450/898 MAE=4.0772 totloss=3.57716703414917\n",
      "  step 500/898 MAE=6.1240 totloss=5.689952373504639\n",
      "  step 550/898 MAE=4.2866 totloss=3.911609172821045\n",
      "  step 600/898 MAE=3.0067 totloss=2.507296562194824\n",
      "  step 650/898 MAE=8.8160 totloss=8.316025733947754\n",
      "  step 700/898 MAE=4.7462 totloss=4.271746635437012\n",
      "  step 750/898 MAE=1.7543 totloss=1.3442132472991943\n",
      "  step 800/898 MAE=3.3577 totloss=2.8580543994903564\n",
      "  step 850/898 MAE=7.0475 totloss=6.656890869140625\n",
      "Epoch 0: train_loss=6.6390, train_MAE=7.091 | val_loss=4.9380, val_MAE=5.378 | full_val_loss=15.3245, full_val_MAE=15.809\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 1 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.8810 totloss=2.3809750080108643\n",
      "  step 50/898 MAE=3.2191 totloss=2.86502742767334\n",
      "  step 100/898 MAE=11.4542 totloss=10.983512878417969\n",
      "  step 150/898 MAE=2.0157 totloss=1.5470209121704102\n",
      "  step 200/898 MAE=18.1675 totloss=17.66751480102539\n",
      "  step 250/898 MAE=2.0466 totloss=1.586133599281311\n",
      "  step 300/898 MAE=3.3995 totloss=3.0764501094818115\n",
      "  step 350/898 MAE=6.9336 totloss=6.460424900054932\n",
      "  step 400/898 MAE=2.4445 totloss=1.9471513032913208\n",
      "  step 450/898 MAE=4.0793 totloss=3.5792999267578125\n",
      "  step 500/898 MAE=1.1244 totloss=0.770793080329895\n",
      "  step 550/898 MAE=4.7541 totloss=4.254111289978027\n",
      "  step 600/898 MAE=2.5200 totloss=2.020007610321045\n",
      "  step 650/898 MAE=3.6310 totloss=3.131086826324463\n",
      "  step 700/898 MAE=4.1645 totloss=3.746030807495117\n",
      "  step 750/898 MAE=12.1734 totloss=11.750683784484863\n",
      "  step 800/898 MAE=6.2871 totloss=5.871589183807373\n",
      "  step 850/898 MAE=4.5111 totloss=4.011071681976318\n",
      "Epoch 1: train_loss=4.2578, train_MAE=4.696 | val_loss=3.9526, val_MAE=4.394 | full_val_loss=11.3498, full_val_MAE=11.835\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 2 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=3.9755 totloss=3.4881880283355713\n",
      "  step 50/898 MAE=0.7112 totloss=0.30821120738983154\n",
      "  step 100/898 MAE=2.5194 totloss=2.2615997791290283\n",
      "  step 150/898 MAE=2.2038 totloss=1.7039830684661865\n",
      "  step 200/898 MAE=5.5643 totloss=5.0643205642700195\n",
      "  step 250/898 MAE=6.3460 totloss=5.846046447753906\n",
      "  step 300/898 MAE=2.6056 totloss=2.135453224182129\n",
      "  step 350/898 MAE=9.1613 totloss=8.661251068115234\n",
      "  step 400/898 MAE=3.3113 totloss=2.8689656257629395\n",
      "  step 450/898 MAE=25.6239 totloss=25.12386703491211\n",
      "  step 500/898 MAE=0.8220 totloss=0.485848605632782\n",
      "  step 550/898 MAE=1.0904 totloss=0.6679472923278809\n",
      "  step 600/898 MAE=3.2726 totloss=2.772613525390625\n",
      "  step 650/898 MAE=5.1213 totloss=4.621259689331055\n",
      "  step 700/898 MAE=1.1429 totloss=0.870463490486145\n",
      "  step 750/898 MAE=1.3268 totloss=0.887462317943573\n",
      "  step 800/898 MAE=1.7613 totloss=1.31644868850708\n",
      "  step 850/898 MAE=2.7052 totloss=2.2273192405700684\n",
      "Epoch 2: train_loss=3.3623, train_MAE=3.791 | val_loss=3.8229, val_MAE=4.257 | full_val_loss=12.0043, full_val_MAE=12.492\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 3 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=3.9384 totloss=3.6015167236328125\n",
      "  step 50/898 MAE=3.7485 totloss=3.459940195083618\n",
      "  step 100/898 MAE=1.3221 totloss=0.8487573266029358\n",
      "  step 150/898 MAE=0.8963 totloss=0.5702654719352722\n",
      "  step 200/898 MAE=1.5667 totloss=1.184685468673706\n",
      "  step 250/898 MAE=4.0984 totloss=3.7233524322509766\n",
      "  step 300/898 MAE=2.4363 totloss=2.04048752784729\n",
      "  step 350/898 MAE=4.1009 totloss=3.6198923587799072\n",
      "  step 400/898 MAE=2.1610 totloss=1.7895317077636719\n",
      "  step 450/898 MAE=4.1454 totloss=3.7201147079467773\n",
      "  step 500/898 MAE=1.6636 totloss=1.392384648323059\n",
      "  step 550/898 MAE=1.7191 totloss=1.4384939670562744\n",
      "  step 600/898 MAE=2.0345 totloss=1.6926250457763672\n",
      "  step 650/898 MAE=5.0801 totloss=4.633893013000488\n",
      "  step 700/898 MAE=1.0784 totloss=0.8120975494384766\n",
      "  step 750/898 MAE=4.3450 totloss=3.9326233863830566\n",
      "  step 800/898 MAE=2.7357 totloss=2.2567052841186523\n",
      "  step 850/898 MAE=3.0376 totloss=2.537588596343994\n",
      "Epoch 3: train_loss=3.0857, train_MAE=3.516 | val_loss=3.0782, val_MAE=3.509 | full_val_loss=8.1426, full_val_MAE=8.612\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 4 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.6294 totloss=2.1559019088745117\n",
      "  step 50/898 MAE=1.6727 totloss=1.3263862133026123\n",
      "  step 100/898 MAE=8.8689 totloss=8.368850708007812\n",
      "  step 150/898 MAE=1.8556 totloss=1.3555911779403687\n",
      "  step 200/898 MAE=1.9730 totloss=1.4892159700393677\n",
      "  step 250/898 MAE=1.3532 totloss=1.0494848489761353\n",
      "  step 300/898 MAE=7.3473 totloss=6.907610893249512\n",
      "  step 350/898 MAE=2.7575 totloss=2.267359495162964\n",
      "  step 400/898 MAE=3.1021 totloss=2.6176252365112305\n",
      "  step 450/898 MAE=5.3879 totloss=4.941566467285156\n",
      "  step 500/898 MAE=9.8914 totloss=9.404047012329102\n",
      "  step 550/898 MAE=1.7682 totloss=1.3908021450042725\n",
      "  step 600/898 MAE=2.1549 totloss=1.7579398155212402\n",
      "  step 650/898 MAE=1.4972 totloss=1.197308897972107\n",
      "  step 700/898 MAE=1.6429 totloss=1.3173671960830688\n",
      "  step 750/898 MAE=1.7598 totloss=1.3221004009246826\n",
      "  step 800/898 MAE=3.7327 totloss=3.257537841796875\n",
      "  step 850/898 MAE=0.2495 totloss=0.07166428864002228\n",
      "Epoch 4: train_loss=2.7246, train_MAE=3.149 | val_loss=3.1547, val_MAE=3.569 | full_val_loss=8.6203, full_val_MAE=9.090\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 5 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.7199 totloss=2.2199041843414307\n",
      "  step 50/898 MAE=6.8609 totloss=6.4130048751831055\n",
      "  step 100/898 MAE=1.6657 totloss=1.2112730741500854\n",
      "  step 150/898 MAE=2.1069 totloss=1.6175189018249512\n",
      "  step 200/898 MAE=3.2370 totloss=2.7548346519470215\n",
      "  step 250/898 MAE=2.4655 totloss=1.9770833253860474\n",
      "  step 300/898 MAE=2.8168 totloss=2.316814661026001\n",
      "  step 350/898 MAE=1.5046 totloss=1.0045726299285889\n",
      "  step 400/898 MAE=2.7056 totloss=2.3066117763519287\n",
      "  step 450/898 MAE=4.4283 totloss=3.9283013343811035\n",
      "  step 500/898 MAE=0.7721 totloss=0.3540297746658325\n",
      "  step 550/898 MAE=4.4491 totloss=3.987729072570801\n",
      "  step 600/898 MAE=3.4330 totloss=2.93302321434021\n",
      "  step 650/898 MAE=1.9957 totloss=1.6156280040740967\n",
      "  step 700/898 MAE=2.7844 totloss=2.340973377227783\n",
      "  step 750/898 MAE=4.6122 totloss=4.154183864593506\n",
      "  step 800/898 MAE=1.9808 totloss=1.5265986919403076\n",
      "  step 850/898 MAE=4.3122 totloss=4.06095027923584\n",
      "Epoch 5: train_loss=2.5846, train_MAE=3.008 | val_loss=3.2178, val_MAE=3.633 | full_val_loss=8.4240, full_val_MAE=8.911\n",
      "  lr=1.00e-04\n",
      "\n",
      "Epoch 6 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.1341 totloss=1.7057907581329346\n",
      "  step 50/898 MAE=3.8544 totloss=3.354375123977661\n",
      "  step 100/898 MAE=1.9617 totloss=1.469267725944519\n",
      "  step 150/898 MAE=1.4357 totloss=1.021872878074646\n",
      "  step 200/898 MAE=2.4220 totloss=1.922003149986267\n",
      "  step 250/898 MAE=0.9321 totloss=0.5719814896583557\n",
      "  step 300/898 MAE=1.1785 totloss=0.6944676041603088\n",
      "  step 350/898 MAE=1.7240 totloss=1.3466765880584717\n",
      "  step 400/898 MAE=2.0702 totloss=1.7655504941940308\n",
      "  step 450/898 MAE=25.7666 totloss=25.266599655151367\n",
      "  step 500/898 MAE=2.2061 totloss=1.7060999870300293\n",
      "  step 550/898 MAE=2.9452 totloss=2.4941070079803467\n",
      "  step 600/898 MAE=2.4894 totloss=2.1629204750061035\n",
      "  step 650/898 MAE=3.1875 totloss=2.687450408935547\n",
      "  step 700/898 MAE=2.0637 totloss=1.5642496347427368\n",
      "  step 750/898 MAE=0.5525 totloss=0.30358681082725525\n",
      "  step 800/898 MAE=1.3011 totloss=0.8247672915458679\n",
      "  step 850/898 MAE=1.3460 totloss=0.8829246759414673\n",
      "Epoch 6: train_loss=2.4339, train_MAE=2.853 | val_loss=3.0313, val_MAE=3.468 | full_val_loss=8.5974, full_val_MAE=9.082\n",
      "  lr=3.00e-05\n",
      "\n",
      "Epoch 7 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.0831 totloss=1.5831172466278076\n",
      "  step 50/898 MAE=2.4737 totloss=1.9737210273742676\n",
      "  step 100/898 MAE=3.4096 totloss=3.1603376865386963\n",
      "  step 150/898 MAE=1.1623 totloss=0.759028434753418\n",
      "  step 200/898 MAE=1.9355 totloss=1.5990667343139648\n",
      "  step 250/898 MAE=1.8391 totloss=1.4273244142532349\n",
      "  step 300/898 MAE=0.9668 totloss=0.5001369118690491\n",
      "  step 350/898 MAE=0.9676 totloss=0.6459447145462036\n",
      "  step 400/898 MAE=6.1116 totloss=5.658890247344971\n",
      "  step 450/898 MAE=2.9522 totloss=2.4521658420562744\n",
      "  step 500/898 MAE=0.9814 totloss=0.6339825391769409\n",
      "  step 550/898 MAE=2.1564 totloss=1.7526419162750244\n",
      "  step 600/898 MAE=0.7428 totloss=0.38750770688056946\n",
      "  step 650/898 MAE=3.1691 totloss=2.7373270988464355\n",
      "  step 700/898 MAE=0.8777 totloss=0.5217449069023132\n",
      "  step 750/898 MAE=2.2709 totloss=1.8823548555374146\n",
      "  step 800/898 MAE=2.1590 totloss=1.6590274572372437\n",
      "  step 850/898 MAE=1.5690 totloss=1.2345205545425415\n",
      "Epoch 7: train_loss=1.6590, train_MAE=2.057 | val_loss=2.5548, val_MAE=2.965 | full_val_loss=6.1188, full_val_MAE=6.591\n",
      "  lr=3.00e-05\n",
      "\n",
      "Epoch 8 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=2.1196 totloss=1.7284903526306152\n",
      "  step 50/898 MAE=0.7322 totloss=0.3016936182975769\n",
      "  step 100/898 MAE=0.7253 totloss=0.463493674993515\n",
      "  step 150/898 MAE=2.5849 totloss=2.0849149227142334\n",
      "  step 200/898 MAE=1.2836 totloss=0.7912148833274841\n",
      "  step 250/898 MAE=1.1884 totloss=0.8133864402770996\n",
      "  step 300/898 MAE=2.3560 totloss=1.9032632112503052\n",
      "  step 350/898 MAE=2.4028 totloss=1.9040578603744507\n",
      "  step 400/898 MAE=1.2284 totloss=0.8168548345565796\n",
      "  step 450/898 MAE=0.7961 totloss=0.47007811069488525\n",
      "  step 500/898 MAE=1.8537 totloss=1.3902649879455566\n",
      "  step 550/898 MAE=3.6302 totloss=3.2551891803741455\n",
      "  step 600/898 MAE=1.4632 totloss=1.0557531118392944\n",
      "  step 650/898 MAE=1.7998 totloss=1.3772732019424438\n",
      "  step 700/898 MAE=0.6637 totloss=0.273093581199646\n",
      "  step 750/898 MAE=1.1265 totloss=0.6707053780555725\n",
      "  step 800/898 MAE=1.5615 totloss=1.1395407915115356\n",
      "  step 850/898 MAE=1.3055 totloss=0.9238101840019226\n",
      "Epoch 8: train_loss=1.4673, train_MAE=1.854 | val_loss=2.3265, val_MAE=2.735 | full_val_loss=6.2346, full_val_MAE=6.705\n",
      "  lr=3.00e-05\n",
      "\n",
      "Epoch 9 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.5733 totloss=0.18679974973201752\n",
      "  step 50/898 MAE=1.2889 totloss=0.9357196092605591\n",
      "  step 100/898 MAE=2.5981 totloss=2.2167415618896484\n",
      "  step 150/898 MAE=1.7015 totloss=1.2993438243865967\n",
      "  step 200/898 MAE=2.7086 totloss=2.2983591556549072\n",
      "  step 250/898 MAE=1.3006 totloss=0.8589814901351929\n",
      "  step 300/898 MAE=0.7067 totloss=0.43470898270606995\n",
      "  step 350/898 MAE=1.8299 totloss=1.3298574686050415\n",
      "  step 400/898 MAE=1.4467 totloss=1.033247470855713\n",
      "  step 450/898 MAE=2.4177 totloss=1.9182850122451782\n",
      "  step 500/898 MAE=1.6474 totloss=1.1615972518920898\n",
      "  step 550/898 MAE=4.3954 totloss=3.9849190711975098\n",
      "  step 600/898 MAE=1.1473 totloss=0.7051271200180054\n",
      "  step 650/898 MAE=0.7429 totloss=0.33384817838668823\n",
      "  step 700/898 MAE=3.1257 totloss=2.660093069076538\n",
      "  step 750/898 MAE=2.0681 totloss=1.696015477180481\n",
      "  step 800/898 MAE=1.6828 totloss=1.2299429178237915\n",
      "  step 850/898 MAE=1.3907 totloss=0.9372740983963013\n",
      "Epoch 9: train_loss=1.3746, train_MAE=1.758 | val_loss=2.3398, val_MAE=2.740 | full_val_loss=6.1821, full_val_MAE=6.656\n",
      "  lr=3.00e-05\n",
      "\n",
      "Epoch 10 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.9937 totloss=0.7437412142753601\n",
      "  step 50/898 MAE=1.0350 totloss=0.6093587875366211\n",
      "  step 100/898 MAE=1.2147 totloss=1.0304981470108032\n",
      "  step 150/898 MAE=0.7633 totloss=0.36829084157943726\n",
      "  step 200/898 MAE=0.5940 totloss=0.289296418428421\n",
      "  step 250/898 MAE=1.0053 totloss=0.5582213997840881\n",
      "  step 300/898 MAE=1.1788 totloss=0.698197603225708\n",
      "  step 350/898 MAE=16.2019 totloss=15.907445907592773\n",
      "  step 400/898 MAE=2.2530 totloss=1.7617778778076172\n",
      "  step 450/898 MAE=1.6478 totloss=1.2270115613937378\n",
      "  step 500/898 MAE=1.0280 totloss=0.5642643570899963\n",
      "  step 550/898 MAE=1.2602 totloss=0.8796355128288269\n",
      "  step 600/898 MAE=1.4777 totloss=0.9884353280067444\n",
      "  step 650/898 MAE=3.0371 totloss=2.6186132431030273\n",
      "  step 700/898 MAE=3.6130 totloss=3.179060935974121\n",
      "  step 750/898 MAE=0.7094 totloss=0.28812694549560547\n",
      "  step 800/898 MAE=0.8040 totloss=0.4640859067440033\n",
      "  step 850/898 MAE=0.7104 totloss=0.3589848577976227\n",
      "Epoch 10: train_loss=1.2931, train_MAE=1.671 | val_loss=2.3443, val_MAE=2.741 | full_val_loss=6.8191, full_val_MAE=7.303\n",
      "  lr=9.00e-06\n",
      "\n",
      "Epoch 11 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.8283 totloss=0.5054693222045898\n",
      "  step 50/898 MAE=0.5732 totloss=0.281037300825119\n",
      "  step 100/898 MAE=0.4262 totloss=0.14155080914497375\n",
      "  step 150/898 MAE=1.2624 totloss=0.7716999053955078\n",
      "  step 200/898 MAE=2.3481 totloss=1.9462227821350098\n",
      "  step 250/898 MAE=0.3837 totloss=0.18338266015052795\n",
      "  step 300/898 MAE=0.5813 totloss=0.20638565719127655\n",
      "  step 350/898 MAE=2.6087 totloss=2.2456576824188232\n",
      "  step 400/898 MAE=0.6896 totloss=0.32877200841903687\n",
      "  step 450/898 MAE=0.3041 totloss=0.07776028662919998\n",
      "  step 500/898 MAE=1.1323 totloss=0.6574058532714844\n",
      "  step 550/898 MAE=0.8116 totloss=0.48890072107315063\n",
      "  step 600/898 MAE=0.4752 totloss=0.22211432456970215\n",
      "  step 650/898 MAE=10.5679 totloss=10.213431358337402\n",
      "  step 700/898 MAE=1.3339 totloss=0.8463742733001709\n",
      "  step 750/898 MAE=4.7823 totloss=4.535852909088135\n",
      "  step 800/898 MAE=0.9498 totloss=0.5649688839912415\n",
      "  step 850/898 MAE=0.5933 totloss=0.25955891609191895\n",
      "Epoch 11: train_loss=1.0030, train_MAE=1.362 | val_loss=2.2505, val_MAE=2.650 | full_val_loss=6.3088, full_val_MAE=6.795\n",
      "  lr=9.00e-06\n",
      "\n",
      "Epoch 12 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.3593 totloss=0.08202429115772247\n",
      "  step 50/898 MAE=1.6661 totloss=1.1777335405349731\n",
      "  step 100/898 MAE=0.4307 totloss=0.1149183139204979\n",
      "  step 150/898 MAE=2.1167 totloss=1.8626383543014526\n",
      "  step 200/898 MAE=1.8283 totloss=1.4618620872497559\n",
      "  step 250/898 MAE=1.8882 totloss=1.5279778242111206\n",
      "  step 300/898 MAE=1.5781 totloss=1.1987224817276\n",
      "  step 350/898 MAE=0.2075 totloss=0.03530785068869591\n",
      "  step 400/898 MAE=3.5733 totloss=3.0732967853546143\n",
      "  step 450/898 MAE=0.9265 totloss=0.5473130941390991\n",
      "  step 500/898 MAE=0.6060 totloss=0.23515349626541138\n",
      "  step 550/898 MAE=0.7667 totloss=0.4227694869041443\n",
      "  step 600/898 MAE=1.7648 totloss=1.3444875478744507\n",
      "  step 650/898 MAE=1.2522 totloss=0.9057048559188843\n",
      "  step 700/898 MAE=0.7537 totloss=0.30968573689460754\n",
      "  step 750/898 MAE=1.4899 totloss=1.0301285982131958\n",
      "  step 800/898 MAE=0.4759 totloss=0.19749104976654053\n",
      "  step 850/898 MAE=0.7657 totloss=0.3426240384578705\n",
      "Epoch 12: train_loss=0.9342, train_MAE=1.288 | val_loss=2.2206, val_MAE=2.615 | full_val_loss=5.9004, full_val_MAE=6.366\n",
      "  lr=9.00e-06\n",
      "\n",
      "Epoch 13 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.8758 totloss=0.44374191761016846\n",
      "  step 50/898 MAE=1.1925 totloss=0.8376010060310364\n",
      "  step 100/898 MAE=0.8572 totloss=0.46723780035972595\n",
      "  step 150/898 MAE=0.3053 totloss=0.06921277195215225\n",
      "  step 200/898 MAE=0.8718 totloss=0.5033021569252014\n",
      "  step 250/898 MAE=1.0384 totloss=0.6836506724357605\n",
      "  step 300/898 MAE=0.6532 totloss=0.25323525071144104\n",
      "  step 350/898 MAE=0.8850 totloss=0.4988362491130829\n",
      "  step 400/898 MAE=1.9652 totloss=1.5398811101913452\n",
      "  step 450/898 MAE=1.4247 totloss=1.0285743474960327\n",
      "  step 500/898 MAE=1.2630 totloss=0.8132625818252563\n",
      "  step 550/898 MAE=4.1940 totloss=3.8581342697143555\n",
      "  step 600/898 MAE=0.2503 totloss=0.0654720887541771\n",
      "  step 650/898 MAE=0.7250 totloss=0.30283960700035095\n",
      "  step 700/898 MAE=0.6402 totloss=0.30615973472595215\n",
      "  step 750/898 MAE=1.2730 totloss=0.7921634912490845\n",
      "  step 800/898 MAE=0.6033 totloss=0.35758793354034424\n",
      "  step 850/898 MAE=2.7943 totloss=2.2987966537475586\n",
      "Epoch 13: train_loss=0.8921, train_MAE=1.240 | val_loss=2.2046, val_MAE=2.605 | full_val_loss=5.8814, full_val_MAE=6.366\n",
      "  lr=9.00e-06\n",
      "\n",
      "Epoch 14 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=1.2929 totloss=0.9525531530380249\n",
      "  step 50/898 MAE=0.8595 totloss=0.5948060154914856\n",
      "  step 100/898 MAE=0.4917 totloss=0.13915905356407166\n",
      "  step 150/898 MAE=1.2301 totloss=0.853624701499939\n",
      "  step 200/898 MAE=0.7182 totloss=0.36139363050460815\n",
      "  step 250/898 MAE=0.6841 totloss=0.3521381914615631\n",
      "  step 300/898 MAE=0.6129 totloss=0.22284002602100372\n",
      "  step 350/898 MAE=0.3490 totloss=0.10859785974025726\n",
      "  step 400/898 MAE=2.8358 totloss=2.367241621017456\n",
      "  step 450/898 MAE=1.3053 totloss=1.0299152135849\n",
      "  step 500/898 MAE=2.3698 totloss=1.876715898513794\n",
      "  step 550/898 MAE=0.4723 totloss=0.19617873430252075\n",
      "  step 600/898 MAE=0.5252 totloss=0.18288981914520264\n",
      "  step 650/898 MAE=0.7182 totloss=0.42782312631607056\n",
      "  step 700/898 MAE=2.3569 totloss=2.023505926132202\n",
      "  step 750/898 MAE=0.4285 totloss=0.12419683486223221\n",
      "  step 800/898 MAE=0.6052 totloss=0.3400512635707855\n",
      "  step 850/898 MAE=1.0574 totloss=0.7083225250244141\n",
      "Epoch 14: train_loss=0.8614, train_MAE=1.207 | val_loss=2.2306, val_MAE=2.630 | full_val_loss=5.9918, full_val_MAE=6.473\n",
      "  lr=9.00e-06\n",
      "\n",
      "Epoch 15 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=1.9614 totloss=1.61577308177948\n",
      "  step 50/898 MAE=0.7092 totloss=0.32939112186431885\n",
      "  step 100/898 MAE=0.7678 totloss=0.32022568583488464\n",
      "  step 150/898 MAE=0.8425 totloss=0.46658390760421753\n",
      "  step 200/898 MAE=0.5904 totloss=0.2036236822605133\n",
      "  step 250/898 MAE=1.1755 totloss=0.7551679611206055\n",
      "  step 300/898 MAE=0.9502 totloss=0.6036059856414795\n",
      "  step 350/898 MAE=0.4710 totloss=0.1306271106004715\n",
      "  step 400/898 MAE=0.2947 totloss=0.07295403629541397\n",
      "  step 450/898 MAE=1.2578 totloss=0.8630009293556213\n",
      "  step 500/898 MAE=0.5423 totloss=0.24448485672473907\n",
      "  step 550/898 MAE=1.2931 totloss=0.9341756105422974\n",
      "  step 600/898 MAE=0.1763 totloss=0.054545097053050995\n",
      "  step 650/898 MAE=1.8403 totloss=1.581079363822937\n",
      "  step 700/898 MAE=0.5877 totloss=0.23345918953418732\n",
      "  step 750/898 MAE=2.2803 totloss=1.8063315153121948\n",
      "  step 800/898 MAE=1.7087 totloss=1.309064507484436\n",
      "  step 850/898 MAE=1.9350 totloss=1.5211400985717773\n",
      "Epoch 15: train_loss=0.8132, train_MAE=1.158 | val_loss=2.2147, val_MAE=2.615 | full_val_loss=5.9148, full_val_MAE=6.391\n",
      "  lr=2.70e-06\n",
      "\n",
      "Epoch 16 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.8725 totloss=0.5410412549972534\n",
      "  step 50/898 MAE=1.6172 totloss=1.2067234516143799\n",
      "  step 100/898 MAE=0.5096 totloss=0.18295007944107056\n",
      "  step 150/898 MAE=0.6665 totloss=0.325064480304718\n",
      "  step 200/898 MAE=8.8870 totloss=8.71357536315918\n",
      "  step 250/898 MAE=0.9745 totloss=0.6125522255897522\n",
      "  step 300/898 MAE=0.9289 totloss=0.45360755920410156\n",
      "  step 350/898 MAE=0.8617 totloss=0.47740405797958374\n",
      "  step 400/898 MAE=1.8236 totloss=1.3820807933807373\n",
      "  step 450/898 MAE=0.5439 totloss=0.23852652311325073\n",
      "  step 500/898 MAE=0.8156 totloss=0.4362037479877472\n",
      "  step 550/898 MAE=0.5220 totloss=0.23765376210212708\n",
      "  step 600/898 MAE=0.2017 totloss=0.06714954972267151\n",
      "  step 650/898 MAE=0.4113 totloss=0.09427626430988312\n",
      "  step 700/898 MAE=1.0165 totloss=0.5378759503364563\n",
      "  step 750/898 MAE=1.5116 totloss=1.011728048324585\n",
      "  step 800/898 MAE=0.2330 totloss=0.049304038286209106\n",
      "  step 850/898 MAE=0.4467 totloss=0.1530197113752365\n",
      "Epoch 16: train_loss=0.7211, train_MAE=1.049 | val_loss=2.2448, val_MAE=2.644 | full_val_loss=6.1535, full_val_MAE=6.642\n",
      "  lr=2.70e-06\n",
      "\n",
      "Epoch 17 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.7762 totloss=0.4904250502586365\n",
      "  step 50/898 MAE=0.8433 totloss=0.5160409808158875\n",
      "  step 100/898 MAE=0.3868 totloss=0.11659423261880875\n",
      "  step 150/898 MAE=1.8409 totloss=1.4788795709609985\n",
      "  step 200/898 MAE=0.5550 totloss=0.2257804274559021\n",
      "  step 250/898 MAE=1.6436 totloss=1.2635306119918823\n",
      "  step 300/898 MAE=0.4258 totloss=0.16714805364608765\n",
      "  step 350/898 MAE=0.6814 totloss=0.2828584909439087\n",
      "  step 400/898 MAE=2.7112 totloss=2.4779257774353027\n",
      "  step 450/898 MAE=0.5008 totloss=0.1763840615749359\n",
      "  step 500/898 MAE=1.3722 totloss=0.9599985480308533\n",
      "  step 550/898 MAE=0.6477 totloss=0.3597119152545929\n",
      "  step 600/898 MAE=1.3206 totloss=0.9998846054077148\n",
      "  step 650/898 MAE=0.9846 totloss=0.5113059878349304\n",
      "  step 700/898 MAE=1.4574 totloss=1.0622164011001587\n",
      "  step 750/898 MAE=0.9864 totloss=0.5566699504852295\n",
      "  step 800/898 MAE=0.3135 totloss=0.059574320912361145\n",
      "  step 850/898 MAE=0.3557 totloss=0.12983062863349915\n",
      "Epoch 17: train_loss=0.7014, train_MAE=1.026 | val_loss=2.2417, val_MAE=2.644 | full_val_loss=6.1146, full_val_MAE=6.589\n",
      "  lr=2.70e-06\n",
      "\n",
      "Epoch 18 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.4800 totloss=0.17319129407405853\n",
      "  step 50/898 MAE=0.5295 totloss=0.25236061215400696\n",
      "  step 100/898 MAE=0.3057 totloss=0.06766307353973389\n",
      "  step 150/898 MAE=1.1437 totloss=0.8787079453468323\n",
      "  step 200/898 MAE=0.6714 totloss=0.32859811186790466\n",
      "  step 250/898 MAE=1.3347 totloss=0.8449039459228516\n",
      "  step 300/898 MAE=1.2072 totloss=0.8506889939308167\n",
      "  step 350/898 MAE=0.5057 totloss=0.14828652143478394\n",
      "  step 400/898 MAE=0.9601 totloss=0.5567881464958191\n",
      "  step 450/898 MAE=0.6255 totloss=0.22044405341148376\n",
      "  step 500/898 MAE=1.4210 totloss=1.0359591245651245\n",
      "  step 550/898 MAE=1.5304 totloss=1.2327135801315308\n",
      "  step 600/898 MAE=0.3934 totloss=0.11271300166845322\n",
      "  step 650/898 MAE=0.3098 totloss=0.0633302628993988\n",
      "  step 700/898 MAE=0.5990 totloss=0.26279786229133606\n",
      "  step 750/898 MAE=0.2030 totloss=0.06008089706301689\n",
      "  step 800/898 MAE=0.7121 totloss=0.5543028712272644\n",
      "  step 850/898 MAE=0.3681 totloss=0.18218861520290375\n",
      "Epoch 18: train_loss=0.6870, train_MAE=1.011 | val_loss=2.1972, val_MAE=2.594 | full_val_loss=5.9304, full_val_MAE=6.405\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 19 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.4529 totloss=0.13935938477516174\n",
      "  step 50/898 MAE=0.7404 totloss=0.43346741795539856\n",
      "  step 100/898 MAE=0.2125 totloss=0.03788638114929199\n",
      "  step 150/898 MAE=1.7488 totloss=1.2791482210159302\n",
      "  step 200/898 MAE=0.7417 totloss=0.4577808082103729\n",
      "  step 250/898 MAE=0.1934 totloss=0.02789244055747986\n",
      "  step 300/898 MAE=0.8893 totloss=0.5450344085693359\n",
      "  step 350/898 MAE=0.0741 totloss=0.0043685538694262505\n",
      "  step 400/898 MAE=0.6191 totloss=0.19888775050640106\n",
      "  step 450/898 MAE=0.8155 totloss=0.42828547954559326\n",
      "  step 500/898 MAE=0.6516 totloss=0.3433913290500641\n",
      "  step 550/898 MAE=0.9383 totloss=0.677878737449646\n",
      "  step 600/898 MAE=1.0443 totloss=0.7082041501998901\n",
      "  step 650/898 MAE=1.0902 totloss=0.7676559090614319\n",
      "  step 700/898 MAE=0.3031 totloss=0.06552959233522415\n",
      "  step 750/898 MAE=0.4698 totloss=0.1921563446521759\n",
      "  step 800/898 MAE=0.4649 totloss=0.2196732759475708\n",
      "  step 850/898 MAE=0.2854 totloss=0.07607100903987885\n",
      "Epoch 19: train_loss=0.6579, train_MAE=0.975 | val_loss=2.2094, val_MAE=2.608 | full_val_loss=5.9840, full_val_MAE=6.458\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 20 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=1.3590 totloss=1.0618218183517456\n",
      "  step 50/898 MAE=0.9091 totloss=0.4772186279296875\n",
      "  step 100/898 MAE=0.6874 totloss=0.2740299701690674\n",
      "  step 150/898 MAE=4.5257 totloss=4.177174091339111\n",
      "  step 200/898 MAE=0.4049 totloss=0.10198909789323807\n",
      "  step 250/898 MAE=0.6348 totloss=0.28771892189979553\n",
      "  step 300/898 MAE=0.2356 totloss=0.04890545457601547\n",
      "  step 350/898 MAE=0.5212 totloss=0.27631476521492004\n",
      "  step 400/898 MAE=0.6290 totloss=0.23371906578540802\n",
      "  step 450/898 MAE=0.2962 totloss=0.06015763804316521\n",
      "  step 500/898 MAE=0.5168 totloss=0.29348891973495483\n",
      "  step 550/898 MAE=0.4872 totloss=0.14358633756637573\n",
      "  step 600/898 MAE=1.4974 totloss=1.055300235748291\n",
      "  step 650/898 MAE=0.2996 totloss=0.08537811785936356\n",
      "  step 700/898 MAE=0.6179 totloss=0.3575510084629059\n",
      "  step 750/898 MAE=2.0452 totloss=1.6867198944091797\n",
      "  step 800/898 MAE=0.3176 totloss=0.11985357850790024\n",
      "  step 850/898 MAE=0.2178 totloss=0.03915225714445114\n",
      "Epoch 20: train_loss=0.6497, train_MAE=0.963 | val_loss=2.2293, val_MAE=2.629 | full_val_loss=6.0801, full_val_MAE=6.562\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 21 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.4517 totloss=0.13267330825328827\n",
      "  step 50/898 MAE=1.4658 totloss=1.075303316116333\n",
      "  step 100/898 MAE=2.1391 totloss=1.7738654613494873\n",
      "  step 150/898 MAE=0.5024 totloss=0.20568576455116272\n",
      "  step 200/898 MAE=0.5889 totloss=0.3142954707145691\n",
      "  step 250/898 MAE=1.0308 totloss=0.6612446308135986\n",
      "  step 300/898 MAE=0.4315 totloss=0.14413169026374817\n",
      "  step 350/898 MAE=0.6012 totloss=0.20229727029800415\n",
      "  step 400/898 MAE=0.5764 totloss=0.21756607294082642\n",
      "  step 450/898 MAE=1.0488 totloss=0.6874914765357971\n",
      "  step 500/898 MAE=0.2911 totloss=0.06547928601503372\n",
      "  step 550/898 MAE=0.1994 totloss=0.0390414223074913\n",
      "  step 600/898 MAE=0.8145 totloss=0.45837369561195374\n",
      "  step 650/898 MAE=0.4356 totloss=0.2000344842672348\n",
      "  step 700/898 MAE=0.2387 totloss=0.03267550840973854\n",
      "  step 750/898 MAE=0.2061 totloss=0.022626854479312897\n",
      "  step 800/898 MAE=4.9603 totloss=4.555027961730957\n",
      "  step 850/898 MAE=1.3648 totloss=1.0494771003723145\n",
      "Epoch 21: train_loss=0.6438, train_MAE=0.957 | val_loss=2.2143, val_MAE=2.615 | full_val_loss=6.0209, full_val_MAE=6.504\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 22 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.7158 totloss=0.36312296986579895\n",
      "  step 50/898 MAE=0.3413 totloss=0.12923435866832733\n",
      "  step 100/898 MAE=0.8162 totloss=0.5063034892082214\n",
      "  step 150/898 MAE=0.3254 totloss=0.06879021227359772\n",
      "  step 200/898 MAE=0.5195 totloss=0.1838407814502716\n",
      "  step 250/898 MAE=0.7912 totloss=0.3804207146167755\n",
      "  step 300/898 MAE=0.4277 totloss=0.1528298407793045\n",
      "  step 350/898 MAE=0.8259 totloss=0.4326087236404419\n",
      "  step 400/898 MAE=0.1437 totloss=0.01428651250898838\n",
      "  step 450/898 MAE=3.7615 totloss=3.5011048316955566\n",
      "  step 500/898 MAE=0.7913 totloss=0.35016191005706787\n",
      "  step 550/898 MAE=1.3967 totloss=1.019566535949707\n",
      "  step 600/898 MAE=0.5773 totloss=0.28894707560539246\n",
      "  step 650/898 MAE=0.8725 totloss=0.4785241484642029\n",
      "  step 700/898 MAE=0.8810 totloss=0.6764539480209351\n",
      "  step 750/898 MAE=2.2761 totloss=1.8375154733657837\n",
      "  step 800/898 MAE=0.4379 totloss=0.11092676222324371\n",
      "  step 850/898 MAE=0.4209 totloss=0.1270895004272461\n",
      "Epoch 22: train_loss=0.6382, train_MAE=0.950 | val_loss=2.2124, val_MAE=2.613 | full_val_loss=5.9822, full_val_MAE=6.462\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 23 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=1.0437 totloss=0.76985102891922\n",
      "  step 50/898 MAE=3.3226 totloss=3.0463273525238037\n",
      "  step 100/898 MAE=0.5551 totloss=0.264532208442688\n",
      "  step 150/898 MAE=0.4559 totloss=0.16657514870166779\n",
      "  step 200/898 MAE=0.0927 totloss=0.006228602025657892\n",
      "  step 250/898 MAE=0.9552 totloss=0.49370095133781433\n",
      "  step 300/898 MAE=0.3427 totloss=0.06425566971302032\n",
      "  step 350/898 MAE=0.4327 totloss=0.10973352193832397\n",
      "  step 400/898 MAE=0.4031 totloss=0.11326874047517776\n",
      "  step 450/898 MAE=0.4011 totloss=0.10026852041482925\n",
      "  step 500/898 MAE=2.4398 totloss=1.979662537574768\n",
      "  step 550/898 MAE=1.3446 totloss=0.8848986625671387\n",
      "  step 600/898 MAE=0.4867 totloss=0.26502755284309387\n",
      "  step 650/898 MAE=0.2159 totloss=0.03179735690355301\n",
      "  step 700/898 MAE=0.6358 totloss=0.29515546560287476\n",
      "  step 750/898 MAE=0.7282 totloss=0.49442434310913086\n",
      "  step 800/898 MAE=0.7601 totloss=0.4778285622596741\n",
      "  step 850/898 MAE=0.9554 totloss=0.5030211210250854\n",
      "Epoch 23: train_loss=0.6340, train_MAE=0.946 | val_loss=2.2174, val_MAE=2.619 | full_val_loss=6.0180, full_val_MAE=6.497\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 24 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=5.1273 totloss=4.727698802947998\n",
      "  step 50/898 MAE=0.1838 totloss=0.02814209833741188\n",
      "  step 100/898 MAE=1.3611 totloss=1.2164647579193115\n",
      "  step 150/898 MAE=2.7008 totloss=2.308605194091797\n",
      "  step 200/898 MAE=0.7407 totloss=0.45603883266448975\n",
      "  step 250/898 MAE=0.6337 totloss=0.30433353781700134\n",
      "  step 300/898 MAE=0.9380 totloss=0.5742256045341492\n",
      "  step 350/898 MAE=0.7175 totloss=0.38525357842445374\n",
      "  step 400/898 MAE=0.2177 totloss=0.0462428517639637\n",
      "  step 450/898 MAE=2.4084 totloss=2.0059807300567627\n",
      "  step 500/898 MAE=0.4022 totloss=0.14745798707008362\n",
      "  step 550/898 MAE=0.3951 totloss=0.08749818801879883\n",
      "  step 600/898 MAE=0.9723 totloss=0.604843258857727\n",
      "  step 650/898 MAE=0.7639 totloss=0.48613718152046204\n",
      "  step 700/898 MAE=1.2887 totloss=0.9046787023544312\n",
      "  step 750/898 MAE=0.6480 totloss=0.36402860283851624\n",
      "  step 800/898 MAE=0.6618 totloss=0.28892266750335693\n",
      "  step 850/898 MAE=1.0182 totloss=0.8137989640235901\n",
      "Epoch 24: train_loss=0.6277, train_MAE=0.938 | val_loss=2.2246, val_MAE=2.625 | full_val_loss=6.0304, full_val_MAE=6.513\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 25 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.3074 totloss=0.06628851592540741\n",
      "  step 50/898 MAE=0.6265 totloss=0.2890750765800476\n",
      "  step 100/898 MAE=0.7462 totloss=0.45801788568496704\n",
      "  step 150/898 MAE=0.7815 totloss=0.40571385622024536\n",
      "  step 200/898 MAE=0.8346 totloss=0.4718819558620453\n",
      "  step 250/898 MAE=0.8439 totloss=0.43644750118255615\n",
      "  step 300/898 MAE=0.5769 totloss=0.33717238903045654\n",
      "  step 350/898 MAE=0.2917 totloss=0.05048797279596329\n",
      "  step 400/898 MAE=1.8228 totloss=1.5173524618148804\n",
      "  step 450/898 MAE=0.2420 totloss=0.03514355048537254\n",
      "  step 500/898 MAE=0.3722 totloss=0.1005592867732048\n",
      "  step 550/898 MAE=0.6436 totloss=0.4274282157421112\n",
      "  step 600/898 MAE=1.4536 totloss=1.0154682397842407\n",
      "  step 650/898 MAE=0.5857 totloss=0.28218185901641846\n",
      "  step 700/898 MAE=0.2824 totloss=0.04464096575975418\n",
      "  step 750/898 MAE=0.8623 totloss=0.6302441358566284\n",
      "  step 800/898 MAE=0.2422 totloss=0.050038885325193405\n",
      "  step 850/898 MAE=0.7148 totloss=0.45467546582221985\n",
      "Epoch 25: train_loss=0.6246, train_MAE=0.935 | val_loss=2.2166, val_MAE=2.616 | full_val_loss=5.9918, full_val_MAE=6.472\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 26 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=3.3543 totloss=3.0404036045074463\n",
      "  step 50/898 MAE=0.4775 totloss=0.14941999316215515\n",
      "  step 100/898 MAE=1.1163 totloss=0.7213534712791443\n",
      "  step 150/898 MAE=0.5440 totloss=0.21518278121948242\n",
      "  step 200/898 MAE=1.7835 totloss=1.4596511125564575\n",
      "  step 250/898 MAE=0.1310 totloss=0.01973024010658264\n",
      "  step 300/898 MAE=0.5927 totloss=0.19410912692546844\n",
      "  step 350/898 MAE=0.4914 totloss=0.15249165892601013\n",
      "  step 400/898 MAE=0.4526 totloss=0.14298249781131744\n",
      "  step 450/898 MAE=3.1041 totloss=2.6315391063690186\n",
      "  step 500/898 MAE=3.2433 totloss=2.899369955062866\n",
      "  step 550/898 MAE=0.6186 totloss=0.2788096070289612\n",
      "  step 600/898 MAE=0.8002 totloss=0.42897021770477295\n",
      "  step 650/898 MAE=1.1822 totloss=0.7606571316719055\n",
      "  step 700/898 MAE=0.8059 totloss=0.37262627482414246\n",
      "  step 750/898 MAE=0.5130 totloss=0.17804473638534546\n",
      "  step 800/898 MAE=0.6309 totloss=0.3214431405067444\n",
      "  step 850/898 MAE=1.4494 totloss=1.0018887519836426\n",
      "Epoch 26: train_loss=0.6170, train_MAE=0.926 | val_loss=2.2284, val_MAE=2.630 | full_val_loss=6.1291, full_val_MAE=6.609\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 27 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=4.9785 totloss=4.567897319793701\n",
      "  step 50/898 MAE=0.2283 totloss=0.032833702862262726\n",
      "  step 100/898 MAE=0.4412 totloss=0.1367747038602829\n",
      "  step 150/898 MAE=0.6222 totloss=0.34267833828926086\n",
      "  step 200/898 MAE=0.4750 totloss=0.24340634047985077\n",
      "  step 250/898 MAE=0.1606 totloss=0.02260526269674301\n",
      "  step 300/898 MAE=0.1704 totloss=0.047371700406074524\n",
      "  step 350/898 MAE=2.9442 totloss=2.5383260250091553\n",
      "  step 400/898 MAE=0.9671 totloss=0.6126418113708496\n",
      "  step 450/898 MAE=0.3496 totloss=0.16260990500450134\n",
      "  step 500/898 MAE=0.6626 totloss=0.38935166597366333\n",
      "  step 550/898 MAE=1.4737 totloss=1.116509199142456\n",
      "  step 600/898 MAE=1.2897 totloss=0.9628771543502808\n",
      "  step 650/898 MAE=1.2862 totloss=0.7966498136520386\n",
      "  step 700/898 MAE=1.2053 totloss=1.0135767459869385\n",
      "  step 750/898 MAE=0.6121 totloss=0.2464664727449417\n",
      "  step 800/898 MAE=1.3384 totloss=0.8849853277206421\n",
      "  step 850/898 MAE=1.1047 totloss=0.8778873085975647\n",
      "Epoch 27: train_loss=0.6132, train_MAE=0.920 | val_loss=2.2166, val_MAE=2.617 | full_val_loss=6.0156, full_val_MAE=6.498\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 28 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.8680 totloss=0.5749529004096985\n",
      "  step 50/898 MAE=0.2411 totloss=0.04228346794843674\n",
      "  step 100/898 MAE=0.5914 totloss=0.2119198739528656\n",
      "  step 150/898 MAE=0.6624 totloss=0.3050251007080078\n",
      "  step 200/898 MAE=0.7884 totloss=0.5224848985671997\n",
      "  step 250/898 MAE=0.6805 totloss=0.4042815864086151\n",
      "  step 300/898 MAE=0.3677 totloss=0.10959489643573761\n",
      "  step 350/898 MAE=2.2501 totloss=1.8365641832351685\n",
      "  step 400/898 MAE=0.4653 totloss=0.1266397088766098\n",
      "  step 450/898 MAE=0.8168 totloss=0.4081888198852539\n",
      "  step 500/898 MAE=1.2066 totloss=0.7250280380249023\n",
      "  step 550/898 MAE=0.9265 totloss=0.5026512145996094\n",
      "  step 600/898 MAE=1.4564 totloss=1.0714648962020874\n",
      "  step 650/898 MAE=0.8179 totloss=0.43075835704803467\n",
      "  step 700/898 MAE=0.6389 totloss=0.29412081837654114\n",
      "  step 750/898 MAE=0.6637 totloss=0.4324300289154053\n",
      "  step 800/898 MAE=0.3511 totloss=0.08531910181045532\n",
      "  step 850/898 MAE=0.4030 totloss=0.08513867110013962\n",
      "Epoch 28: train_loss=0.6081, train_MAE=0.915 | val_loss=2.2359, val_MAE=2.636 | full_val_loss=6.0856, full_val_MAE=6.569\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 29 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.4427 totloss=0.10619410872459412\n",
      "  step 50/898 MAE=0.4063 totloss=0.1251344084739685\n",
      "  step 100/898 MAE=0.5005 totloss=0.17004893720149994\n",
      "  step 150/898 MAE=1.6333 totloss=1.3080554008483887\n",
      "  step 200/898 MAE=0.2769 totloss=0.04794464260339737\n",
      "  step 250/898 MAE=1.4069 totloss=1.0169774293899536\n",
      "  step 300/898 MAE=0.5222 totloss=0.22493544220924377\n",
      "  step 350/898 MAE=0.8689 totloss=0.5246561169624329\n",
      "  step 400/898 MAE=7.2281 totloss=6.728147506713867\n",
      "  step 450/898 MAE=0.9075 totloss=0.6007171273231506\n",
      "  step 500/898 MAE=1.1110 totloss=0.7017542123794556\n",
      "  step 550/898 MAE=0.2189 totloss=0.05899626761674881\n",
      "  step 600/898 MAE=1.0289 totloss=0.790482759475708\n",
      "  step 650/898 MAE=0.7954 totloss=0.4361118674278259\n",
      "  step 700/898 MAE=0.4981 totloss=0.2775253653526306\n",
      "  step 750/898 MAE=0.2915 totloss=0.09032777696847916\n",
      "  step 800/898 MAE=0.8901 totloss=0.552343487739563\n",
      "  step 850/898 MAE=0.3912 totloss=0.14683443307876587\n",
      "Epoch 29: train_loss=0.6037, train_MAE=0.910 | val_loss=2.2178, val_MAE=2.619 | full_val_loss=6.0311, full_val_MAE=6.513\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 30 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=1.2775 totloss=0.9245262742042542\n",
      "  step 50/898 MAE=1.1297 totloss=0.7302412390708923\n",
      "  step 100/898 MAE=1.0649 totloss=0.5980844497680664\n",
      "  step 150/898 MAE=0.6023 totloss=0.3501106798648834\n",
      "  step 200/898 MAE=0.2732 totloss=0.06583309173583984\n",
      "  step 250/898 MAE=1.0748 totloss=0.7075104713439941\n",
      "  step 300/898 MAE=5.7950 totloss=5.480591297149658\n",
      "  step 350/898 MAE=0.7603 totloss=0.4335596561431885\n",
      "  step 400/898 MAE=1.1120 totloss=0.8868830800056458\n",
      "  step 450/898 MAE=0.5376 totloss=0.2718510031700134\n",
      "  step 500/898 MAE=0.9166 totloss=0.4849150776863098\n",
      "  step 550/898 MAE=0.8625 totloss=0.5736016035079956\n",
      "  step 600/898 MAE=0.5404 totloss=0.16771115362644196\n",
      "  step 650/898 MAE=1.0177 totloss=0.767536997795105\n",
      "  step 700/898 MAE=1.1846 totloss=0.6851183772087097\n",
      "  step 750/898 MAE=0.2289 totloss=0.04248606041073799\n",
      "  step 800/898 MAE=0.4119 totloss=0.11407509446144104\n",
      "  step 850/898 MAE=1.7468 totloss=1.4756948947906494\n",
      "Epoch 30: train_loss=0.5977, train_MAE=0.902 | val_loss=2.2214, val_MAE=2.622 | full_val_loss=6.0973, full_val_MAE=6.579\n",
      "  lr=1.00e-06\n",
      "\n",
      "Epoch 31 START\n",
      "  First train batch loaded: torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "  step 0/898 MAE=0.7590 totloss=0.36335790157318115\n",
      "  step 50/898 MAE=0.4827 totloss=0.16534367203712463\n",
      "  step 100/898 MAE=1.3595 totloss=1.092299461364746\n",
      "  step 150/898 MAE=0.2408 totloss=0.05485577508807182\n",
      "  step 200/898 MAE=0.3701 totloss=0.08352811634540558\n",
      "  step 250/898 MAE=0.8092 totloss=0.4582574665546417\n",
      "  step 300/898 MAE=2.2599 totloss=1.8581405878067017\n",
      "  step 350/898 MAE=0.7568 totloss=0.38849732279777527\n",
      "  step 400/898 MAE=1.1021 totloss=0.851094126701355\n",
      "  step 450/898 MAE=0.4656 totloss=0.19804517924785614\n",
      "  step 500/898 MAE=0.8395 totloss=0.49885907769203186\n",
      "  step 550/898 MAE=1.5616 totloss=1.1698864698410034\n",
      "  step 600/898 MAE=1.9442 totloss=1.5682594776153564\n",
      "  step 650/898 MAE=0.3734 totloss=0.09873918443918228\n",
      "  step 700/898 MAE=0.2742 totloss=0.05573415011167526\n",
      "  step 750/898 MAE=0.3812 totloss=0.12429063022136688\n",
      "  step 800/898 MAE=0.7180 totloss=0.40247684717178345\n",
      "  step 850/898 MAE=1.1226 totloss=0.7404029369354248\n"
     ]
    }
   ],
   "source": [
    "model = UNetLiteDensity(in_channels=1, base=16, out_activation=\"relu\", patch_size=P)\n",
    "\n",
    "loss_fn = total_loss\n",
    "stride = 256\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.3, patience=2, min_lr=1e-6\n",
    ")\n",
    "\n",
    "def count_mae(pred, y):\n",
    "    pred_c = pred.sum(dim=(1,2,3))\n",
    "    true_c = y.sum(dim=(1,2,3))\n",
    "    return (pred_c - true_c).abs().mean()\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "print(\"Train dataset len:\", len(train_dataset), flush=True)\n",
    "print(\"Val dataset len:\", len(val_dataset), flush=True)\n",
    "print(\"Train batches per epoch:\", len(train_loader), flush=True)\n",
    "print(\"Val batches per epoch:\", len(val_loader), flush=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "full_val_losses = []\n",
    "\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "full_val_maes = []\n",
    "\n",
    "train_epoch_nums = []\n",
    "val_epoch_nums = []\n",
    "\n",
    "hyperparameters = {\"lr\": lr, \n",
    "                   \"weight_decay\": weight_decay}\n",
    "print(hyperparameters)\n",
    "\n",
    "max_norm = 5.0\n",
    "clip_count = 0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch} START\", flush=True)\n",
    "    model.train()\n",
    "    train_loss_acc = 0.0\n",
    "    train_mae_acc = 0.0\n",
    "    nb = 0\n",
    "\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        if step == 0:\n",
    "            print(\"  First train batch loaded:\", x.shape, y.shape, flush=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        if grad_norm.item() > max_norm:\n",
    "            clip_count += 1\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        train_loss_acc += train_loss\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_avg_mae = count_mae(pred, y).item()\n",
    "        train_mae_acc += train_avg_mae\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                cnt = count_huber(pred, y).item()\n",
    "\n",
    "            print(\n",
    "                f\"  step {step}/{len(train_loader)} \"\n",
    "                f\"MAE={train_avg_mae:.4f} totloss={train_loss}\",\n",
    "                flush=True\n",
    "            )\n",
    "\n",
    "        nb += 1\n",
    "        \n",
    "\n",
    "    train_loss = train_loss_acc / nb\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_mae = train_mae_acc / nb\n",
    "    train_maes.append(train_mae)\n",
    "\n",
    "    train_epoch_nums.append(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_acc = 0.0\n",
    "    val_mae_acc = 0.0\n",
    "    nb = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in enumerate(val_loader):\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            val_loss = loss.item()\n",
    "            val_loss_acc += val_loss\n",
    "\n",
    "            val_avg_mae = count_mae(pred, y).item()\n",
    "            val_mae_acc += val_avg_mae\n",
    "\n",
    "            nb += 1\n",
    "\n",
    "    val_loss = val_loss_acc / nb\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_mae = val_mae_acc / nb\n",
    "    val_maes.append(val_mae)\n",
    "\n",
    "    val_epoch_nums.append(epoch)\n",
    "\n",
    "    full_val_loss_acc = 0.0\n",
    "    full_val_mae_acc = 0.0\n",
    "    nb_full = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_id in val_ids:\n",
    "            x_full = np.load(os.path.join(root, val_id, \"image.npy\"), mmap_mode=\"r\").astype(np.float32)\n",
    "            y_full = np.load(os.path.join(root, val_id, \"density_varfactor0.2.npy\")).astype(np.float32)\n",
    "\n",
    "            H, W = x_full.shape\n",
    "            H_pad = ((H + P - 1) // P) * P\n",
    "            W_pad = ((W + P - 1) // P) * P\n",
    "\n",
    "            x_pad_full = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "            x_pad_full[:H, :W] = x_full\n",
    "\n",
    "            pred_count_total = 0.0\n",
    "            for top in range(0, H_pad, P):\n",
    "                for left in range(0, W_pad, P):\n",
    "                    x_patch = x_pad_full[top:top+P, left:left+P]\n",
    "                    xt = torch.from_numpy(x_patch)[None, None, :, :]\n",
    "                    pred_patch = model(xt).item()\n",
    "                    pred_count_total += pred_patch\n",
    "            \n",
    "            y_full_4d = torch.from_numpy(y_full).unsqueeze(0).unsqueeze(0)\n",
    "            pred_full_4d = torch.tensor(pred_count_total, dtype=torch.float32).view(1,1,1,1)\n",
    "\n",
    "            loss = loss_fn(pred_full_4d, y_full_4d)\n",
    "            \n",
    "            full_val_loss_acc += loss.item()\n",
    "\n",
    "            full_val_avg_mae = count_mae(pred_full_4d, y_full_4d).item()\n",
    "            full_val_mae_acc += full_val_avg_mae\n",
    "            \n",
    "\n",
    "            nb_full += 1\n",
    "\n",
    "    full_val_loss = full_val_loss_acc / nb_full\n",
    "    full_val_losses.append(full_val_loss)\n",
    "\n",
    "    full_val_mae = full_val_mae_acc / nb_full\n",
    "    full_val_maes.append(full_val_mae)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_MAE={train_mae:.3f} | val_loss={val_loss:.4f}, val_MAE={val_mae:.3f} | full_val_loss={full_val_loss:.4f}, full_val_MAE={full_val_mae:.3f}\")\n",
    "    scheduler.step(full_val_mae)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"  lr={current_lr:.2e}\", flush=True)\n",
    "\n",
    "print(\"epoch clip_count:\", clip_count, flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
